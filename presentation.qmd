---
title: 
  "Bayesian Functional Optimization"
subtitle:
  "STAT 521B Paper Presentation"
author: 
  "Joey Hotz"
date: 
  today
date-format:
  "[Last Updated:] DD MMMM YYYY"
format: 
  revealjs:
    slide-number: false
    theme: [default, custom.scss]
    include-after-body: 
      text: |
        <style>
        .reveal .slide ul {
        margin-bottom: 0;
        }
        </style>
editor: 
  source
---

```{r setup, warning = FALSE, comment = FALSE, message = FALSE}
# load packages
library(knitr)
library(kableExtra)
library(tidyverse)
```

## Table of Contents

- Standard Bayesian Optimization
- Bayesian Functional Optimization
- 

# [Standard Bayesian Optimization]{style="color:#fafafa;"} {background-color=#005500}

## Objectives

::: {.fragment fragment-index=1}
- We have an unknown [black-box]{style="color:#ff9900"} real-valued function $\tilde{f}$ defined on a closed and bounded subset $\mathcal{X} \subseteq \mathbb{R}^{d}$.
:::
:::: {.fragment fragment-index=2}
- At step $n = 1, 2, \dots$, we select a value $x_{n} \in \mathcal{X}$ and we get a *noisy* observation $y_{n} = \tilde{f}(x_{n}) + \epsilon_{n}$.
  * There is some cost associated with evaluating $\tilde{f}$ to acquire new data.
::::
::: {.fragment fragment-index=3}
- Goal: **Find the [global maximum]{style="color:#ff9900"}** $x^{*} = \textrm{argmax}_{x \in \mathcal{X}}\tilde{f}(x)$.
:::

## Gaussian Processes

::: {.incremental}
- A common model for Bayesian optimization is to perform regression using [Gaussian processes]{style="color:#ff9900"}.
- A Gaussian process (GP) is a stochastic process which is modelled using multivariate Normal distributions.
  * A GP is fully specified by its mean $\mu(\cdot)$ and covariance $k(\cdot, \cdot)$.
- For Bayesian optimization, we assume $f(x) \sim \mathcal{GP}(\mu(x), k(x, \cdot))$.
:::

## Gaussian Process Example

::: {.fragment fragment-index=1}
To demonstrate a GP, we will define the following "unknown" smooth function $\tilde{f}(x)$ on the interval $[0,10]$:
$$
\tilde{f}(x) = \frac{1}{8}\sin^{2}(\pi{x})\sqrt{x+4}-\sin\Big(\frac{\pi{x}}{2}\Big)\cos\Big(\frac{\pi{x^2}}{7}\Big)
$$
:::
::: {.fragment fragment-index=2}
![](images/true_example_fn.jpg){.nostretch fig-align="center" width="500px"}
:::

## Gaussian Process Example

::: {.r-stack}

:::: {.fragment .fade-in fragment-index=1}
::: {.fragment .fade-out fragment-index=2}
![](images/GP_prior.jpg){.nostretch fig-align="center" width=85%}
:::
::::
:::: {.fragment .fade-in fragment-index=2}
::: {.fragment .fade-out fragment-index=3}
![](images/GP1_noise_high.jpg){.nostretch fig-align="center" width=85%}
:::
::::
:::: {.fragment .fade-in fragment-index=3}
::: {.fragment .fade-out fragment-index=4}
![](images/GP2_noise_high.jpg){.nostretch fig-align="center" width=85%}
:::
::::
:::: {.fragment .fade-in fragment-index=4}
::: {.fragment .fade-out fragment-index=5}
![](images/GP5_noise_high.jpg){.nostretch fig-align="center" width=85%}
:::
::::
:::: {.fragment .fade-in fragment-index=5}
::: {.fragment .fade-out fragment-index=6}
![](images/GP10_noise_high.jpg){.nostretch fig-align="center" width=85%}
:::
::::
:::: {.fragment .fade-in fragment-index=6}
![](images/GP20_noise_high.jpg){.nostretch fig-align="center" width=85%}
::::
:::

## Acquisition Functions

:::: {.r-stack}
:::: {.fragment .fade-out fragment-index=3}
::: {.fragment .fade-in fragment-index=1}
- In Bayesian optimization, we sequentially choose points $x_{n} \in \mathcal{X}$ where we want to evaluate $f(x_{n})$.
:::
::: {.fragment .fade-in fragment-index=2}
- There are three common decision rules for selecting $x_{n+1}$:
  * Probability of Improvement
  * Expected Improvement
  * Gaussian Process Upper Confidence Bound (GP-UCB)
:::
::::
::: {.fragment .fade-in fragment-index=4}
![](images/GP20_noise_high.jpg){.absolute left=0 top=200 width=47.5%}
:::
::: {.fragment .fade-in fragment-index=4}
![](images/acquisition_plots.jpg){.absolute right=0 top=200 width=47.5%}
:::
::::

# [Bayesian Functional Optimization]{style="color:#fafafa;"} {background-color=#005500}

## Reproducing Kernel Hilbert Spaces

::: {.fragment fragment-index=1}
- A Hilbert space $\mathcal{H}$ of functions $\mathcal{X} \longrightarrow \mathbb{R}$ with inner product $\langle\cdot,\cdot\rangle_{\mathcal{H}}$ is an [RKHS]{style="color:#ff9900"} with *kernel* $K: \mathcal{X} \times \mathcal{X} \longrightarrow \mathbb{R}$ if it satisfies the following:
  
  ::: {.fragment fragment-index=2}
  - For every fixed $\mathbf{x} \in \mathcal{X}$, the function $L_{\mathbf{x}}(\cdot) = K(\cdot, \mathbf{x})$ is in $\mathcal{H}$.
  :::
  
  ::: {.fragment fragment-index=2}
  - [Reproducing Property]{style="color:#ff9900"}: For all $f \in \mathcal{H}$ and $\mathbf{x} \in \mathcal{X}$, $\langle{f, L_{\mathbf{x}}}\rangle_{\mathcal{H}} = f(\mathbf{x})$.
  :::
  
:::: {.fragment fragment-index=3}
- For Bayesian Functional Optimization, we assume that $\tilde{f}$ is in some RKHS $\mathcal{H}_{K}$ with corresponding kernel $K$.
::::
:::

<!-- ## Applications -->

<!-- ## Simulations -->

<!-- For our example simulations, we define the following "unknown" smooth function $\tilde{f}(x)$ on the interval $[0,10]$: -->
<!-- $$ -->
<!-- \tilde{f}(x) = \frac{1}{8}\sin^{2}(\pi{x})\sqrt{x+4}-\sin\Big(\frac{\pi{x}}{2}\Big)\cos\Big(\frac{\pi{x^2}}{7}\Big) -->
<!-- $$ -->

## Works Cited

- https://cdn.aaai.org/ojs/11830/11830-13-15358-1-2-20201228.pdf
- https://bookdown.org/rbg/surrogates/chap5.html
- https://bayesoptbook.com/book/bayesoptbook_letter.pdf
- https://gaussianprocess.org/gpml/chapters/RW.pdf
- https://krasserm.github.io/2018/03/19/gaussian-processes/
